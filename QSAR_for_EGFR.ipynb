{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **QSAR for EGFR**\n",
        "The Epidermal Growth Factor Receptor (**EGFR**) is a transmembrane protein and member of the receptor tyrosine kinase family that plays a crucial role in regulating cell growth, survival, proliferation, and differentiation.Dysregulation of EGFR is implicated in the development and progression of various cancers, including non-small cell lung cancer, colorectal cancer, and glioblastoma. EGFR has become an important therapeutic target, with several small-molecule inhibitors and monoclonal antibodies developed to block its signaling activity in cancer cells.\n",
        "\n",
        "In this tutorial, we will use **QSAR  modeling to predict the biological activity (pIC50) of small molecules targeting EGFR**, based on their molecular structure."
      ],
      "metadata": {
        "id": "p07j3QmwNuGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "id": "xUB3R5nyMLAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import  GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold, RFECV\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "aWkgJa_MBuGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First getting data:"
      ],
      "metadata": {
        "id": "a-M-HQem-Taw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YXV8p4iMv1M"
      },
      "outputs": [],
      "source": [
        "file_name = 'https://raw.githubusercontent.com/SafiehLadani/QSAR-quantitative-structure-activity-relationship/main/data/chembl_data_1.csv'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(file_name)\n",
        "data"
      ],
      "metadata": {
        "id": "8GpPNd6AB8UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same compound, identified by its **ChEMBL ID**, may have multiple reported activities due to differences in assays, measurement techniques, or experimental conditions.\n",
        "\n",
        "By grouping the data based on ChEMBL_ID and computing the mean of the pIC50 values, we obtain a single representative activity value for each unique compound."
      ],
      "metadata": {
        "id": "j42HiJOYKMyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.groupby('ChEMBL_ID', as_index=False).agg({\n",
        "    'SMILES': 'first', # keep the first SMILES\n",
        "    'pIC50': 'mean'\n",
        "})\n",
        "df"
      ],
      "metadata": {
        "id": "fxfvKI9lItKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generating  descriptors**\n",
        "Remember that machine learning models require numerical features as input. SMILES strings, while useful as a chemical representation, are not directly interpretable by algorithms—they are just text. The descriptors capture the underlying chemical and structural characteristics that influence a molecule's biological activity."
      ],
      "metadata": {
        "id": "p9CpYj0YMgf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_descriptors(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol:\n",
        "        return {\n",
        "            'MolWt': Descriptors.MolWt(mol),\n",
        "            'MolLogP': Descriptors.MolLogP(mol),\n",
        "            'NumHDonors': Descriptors.NumHDonors(mol),\n",
        "            'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
        "            'TPSA': Descriptors.TPSA(mol),\n",
        "            'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),\n",
        "            'RingCount': Descriptors.RingCount(mol),\n",
        "            'FractionCSP3': Descriptors.FractionCSP3(mol),\n",
        "            'HeavyAtomCount': Descriptors.HeavyAtomCount(mol),\n",
        "            'NHOHCount': Descriptors.NHOHCount(mol),\n",
        "        }\n",
        "    return None\n",
        "\n",
        "desc_list = df['SMILES'].apply(compute_descriptors)\n",
        "desc_df = pd.DataFrame([x for x in desc_list if x is not None])\n",
        "df_clean = df[desc_list.notnull()].reset_index(drop=True)\n",
        "df_final = pd.concat([df_clean, desc_df], axis=1)\n",
        "df_final.head()"
      ],
      "metadata": {
        "id": "QinYW9HZMg_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Selection**\n",
        "Feature selection involves identifying the **most relevant molecular descriptors** that have the greatest impact on a target outcome, such as biological activity or toxicity. By selecting only the most informative features, researchers can reduce model complexity, improve prediction accuracy, and minimize overfitting.\n",
        "\n",
        "To identify the subset of features that contribute most to predictive power, a **hybrid strategy** is often the most effective. This involves starting with **filter-based methods** to remove descriptors that have low variance or are highly correlated with others ( thereby reducing redundancy.) Then, more **refined model-based techniques**, such as recursive feature elimination with cross-validation **(RFECV)** or feature importance ranking from tree-based models (e.g., Random Forest).\n",
        "\n",
        "First X and y are defined:"
      ],
      "metadata": {
        "id": "jpQtoCfLPXEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_final.drop(columns=['ChEMBL_ID', 'SMILES', 'pIC50'])  # Keep only descriptors\n",
        "y = df_final['pIC50']"
      ],
      "metadata": {
        "id": "bE7rsUh9PYpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Scaling**\n",
        "Feature scaling ensures all molecular descriptors contribute equally to the model by transforming them to a similar range or distribution. Without scaling, features with larger numeric ranges (like molecular weight) can dominate others (like LogP), leading to biased or unstable models"
      ],
      "metadata": {
        "id": "EnUdsXuS91sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
      ],
      "metadata": {
        "id": "RB94mIRB92EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filter method to remove low-variance features**\n",
        "\n",
        "Features with low variance carry little information.\n",
        "For example, if a feature has nearly the same value for every molecule, it won’t help a model distinguish between different outputs (e.g., activity, toxicity).\n",
        "\n",
        "Here the `VarianceThreshold` method is initialized with a threshold of 0.01, meaning any feature in X with variance less than 0.01 will be excluded."
      ],
      "metadata": {
        "id": "nn1sS3VHP58O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"before: {X.shape}\")\n",
        "var_thresh = VarianceThreshold(threshold=0.05)\n",
        "X_var = var_thresh.fit_transform(X)\n",
        "X_var_df = X.iloc[:, var_thresh.get_support(indices=True)]\n",
        "print(f\"after: {X_var_df.shape}\")"
      ],
      "metadata": {
        "id": "EiL18A7NlS87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filter method to remove highly correlated features**\n",
        "\n",
        "Highly correlated features are removed to reduce multicollinearity, a condition where two or more features in a dataset are strongly  related."
      ],
      "metadata": {
        "id": "jh2O7ZWupJ2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = X_var_df.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, square=True,\n",
        "            cbar_kws={\"shrink\": 0.5}, linewidths=0.5)\n",
        "plt.title(\"Correlation Matrix of Molecular Descriptors\", fontsize=14)\n",
        "plt.xticks(rotation=90)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wOr2-vnWmgpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the correlation matrix, **MolWt–HeavyAtomCount** (0.96) and **NumHDonors–NHOHCount** (0.95) show strong multicollinearity. Such high correlation can introduce redundancy and reduce model reliability. Removing one feature from each pair helps improve model performance and interpretability.\n"
      ],
      "metadata": {
        "id": "9TAPyZMP5Xu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"before drop: {X_var_df.shape}\")\n",
        "to_drop = [col for col in upper.columns if any(upper[col] > 0.9)]\n",
        "X_filtered = X_var_df.drop(columns=to_drop)\n",
        "print(f\"after drop: {X_filtered.shape}\")"
      ],
      "metadata": {
        "id": "f1xswQfymgkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Splitting Data**"
      ],
      "metadata": {
        "id": "NrZmKYDQAYgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Hzepko-Jmgha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizing Feature Selection with RFECV and Random Forest**\n",
        "Recursive Feature Elimination with Cross-Validation (**RFECV**) is an advanced feature selection technique that systematically identifies the most relevant input variables for a predictive model. When combined with a **Random Forest regressor**, RFECV leverages the model’s inherent ability to rank features by importance while using **cross-validation** to evaluate performance at each step. The process recursively removes the least important features, retraining the model multiple times, and selects the **optimal subset of features based on the cross-validated performance metric**.\n",
        "\n",
        "The **RFECV object** is configured to eliminate one feature at a time (step=1) and evaluate model performance using the **negative mean squared error** as the scoring metric. The model is then trained on the training data, and the most relevant features are selected based on cross-validated performance."
      ],
      "metadata": {
        "id": "az_kvp_dDClB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "selector = RFECV(rf, step=1, cv=cv, scoring=\"neg_mean_squared_error\")\n",
        "selector.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "gHVwiQcqDDBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Select optimal features**\n",
        "The **feature selector** trained by RFECV is now applied to transform the original training and test datasets. The **`transform()`** method retains only the most relevant features identified during cross-validation, effectively reducing the dimensionality of the input data."
      ],
      "metadata": {
        "id": "sPKjTBEGDDVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_sel = selector.transform(X_train)\n",
        "X_test_sel = selector.transform(X_test)"
      ],
      "metadata": {
        "id": "QygL1pJyNTjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Final model training and evaluation**\n",
        "In this final step, a **Random Forest model** is trained using the optimal features selected through **RFECV**. The model is fitted to the **reduced training set (X_train_sel)**, and predictions are made on the **reduced test set (X_test_sel)**. The model's performance is then evaluated by calculating the Mean Squared Error (**MSE**),"
      ],
      "metadata": {
        "id": "HSa_C7nLN5eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_final = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_final.fit(X_train_sel, y_train)\n",
        "y_pred = rf_final.predict(X_test_sel)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Test MSE:\", round(mse, 3))\n",
        "print(f\"y_train statistics:\\n {y_train.describe()}\")"
      ],
      "metadata": {
        "id": "0zLDJkvsN8Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test **MSE of 1.024** is relatively high, especially because our target variable (pIC50) ranges between 2 and 11 with a mean around 6. This may indicate that the model is not generalizing well to unseen data, even though feature selection was performed using **RFECV**. A possible reason is **overfitting—the** model may have learned the training data too well but failed to capture the underlying general patterns.\n",
        "\n",
        "### To improve performance, several strategies can be explored—but first, let's examine what happens when we skip feature selection entirely."
      ],
      "metadata": {
        "id": "wsNS_51wlsRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "D88aE-FPoRu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_final = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_final.fit(X_train, y_train)\n",
        "y_pred = rf_final.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Test MSE:\", round(mse, 3))\n"
      ],
      "metadata": {
        "id": "aBtru3JJoc-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simple model without feature selection shows reasonable performance using default hyperparameters. When compared to the previous model with **RFECV (MSE = 1.024)**, it actually performs better, indicating that:\n",
        "\n",
        "* RFECV may have removed important predictive features, reducing model effectiveness.\n",
        "\n",
        "* The feature selection process might have been too aggressive or unstable, especially with limited data.\n",
        "\n",
        "Sometimes, retaining all features allows the Random Forest to leverage its inherent ability to handle irrelevant or less important variables, which may lead to better generalization."
      ],
      "metadata": {
        "id": "Y7Fx5lInpjFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do a comparision with the Random Forest model, two other ensemble regression models—Gradient Boosting and XGBoost—are used to evaluate performance on the dataset. Each model is configured with 100 estimators while the boosting models are assigned a learning rate of 0.1."
      ],
      "metadata": {
        "id": "RLp_ma7OvnB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "\n",
        "models = {'Random Forest': rf, 'Gradient Boosting': gb, 'XGBoost': xgb}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"{name} Test MSE: {round(mse, 3)}\")\n"
      ],
      "metadata": {
        "id": "uLzbUcmZrpnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comparison shows that **Random Forest** still shows the **lowest test MSE (0.938)**, outperforming both Gradient Boosting (1.023) and XGBoost (0.992) on this dataset. This suggests that, under the current settings and without hyperparameter tuning, Random Forest is better suited for capturing the patterns in the data. Gradient Boosting and XGBoost, while powerful, may require more careful tuning of parameters such as learning rate, tree depth, and regularization to reach their full potential.\n",
        "\n",
        "These results highlight the importance of starting with a baseline model like Random Forest and using it as a benchmark before investing time in optimizing more complex models."
      ],
      "metadata": {
        "id": "YNrx2Fd6r8ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implications for drug design:**\n",
        "The pIC50 values are  log-transformed concentrations, so a difference of 1 unit represents a 10-fold difference in the underlying IC50 concentration.\n",
        "\n",
        "The **RMSE of 0.968** (from the MSE = 0.938)means our model's predictions are, on average, off by roughly a factor of 10 in terms of the actual IC50 values. For example, if the true pIC50 is 7, our model might predict 6 or 8. This translates to the actual concentration being 10 times higher or lower than predicted.\n",
        "\n",
        "In drug discovery, a factor of 10 difference can be quite significant for **lead optimization**, compound prioritization, or understanding structure-activity relationships.\n",
        "While not disastrous for initial screening, for fine-tuning or predicting precise activity, this error might be considered high."
      ],
      "metadata": {
        "id": "3rD3WSNhtZ0K"
      }
    }
  ]
}